{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spooky author identification challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This challenge invites kagglers to identify the horror story author from the given text snippets. The challenge is little different from other nlp problems because we need to find the signature of the author from his writing style than simply understanding the context vectors. \n",
    "\n",
    "### Hence word vectors may be of little help here. I am using keras embeddings instead of word2vec and combine features from LSTM and CNN(to be able to find pattern translation) and then pass the combined features to another hidden layer in the neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, TimeDistributed\n",
    "from keras.layers import Flatten, Reshape\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import optimizers\n",
    "from keras.backend import flatten\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>While I was thinking how I should possibly man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>I am not sure to what limit his knowledge may ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...\n",
       "1  id24541  If a fire wanted fanning, it could readily be ...\n",
       "2  id00134  And when they had broken down the frail door t...\n",
       "3  id27757  While I was thinking how I should possibly man...\n",
       "4  id04081  I am not sure to what limit his knowledge may ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "y = lbl_enc.fit_transform(train.author.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtrain, xvalid, ytrain, yvalid = train_test_split(train.text.values, y, \n",
    "                                                  stratify=y, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# binarize the labels for neural net\n",
    "from keras.utils import np_utils\n",
    "ytrain_enc = np_utils.to_categorical(ytrain)\n",
    "yvalid_enc = np_utils.to_categorical(yvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "xtrain_enc = [one_hot(d, vocab_size) for d in xtrain]\n",
    "xvalid_enc = [one_hot(d, vocab_size) for d in xvalid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_length = 300\n",
    "padded_docs_train = pad_sequences(xtrain_enc, maxlen=max_length, padding='post')\n",
    "padded_docs_valid = pad_sequences(xvalid_enc, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main_input (InputLayer)         (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 300, 300)     3000000     main_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 100)          160400      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 9600)         48032       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 9700)         0           lstm_1[0][0]                     \n",
      "                                                                 sequential_1[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 300)          2910300     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 3)            903         dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 6,119,635\n",
      "Trainable params: 6,119,635\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "## Create a functional api for shared features from cnn and lstm\n",
    "\n",
    "#lstm with cnn\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "main_input = Input(shape=(300,), dtype='int32', name='main_input')\n",
    "\n",
    "# embedding vectors\n",
    "x = Embedding(vocab_size, 300, input_length=max_length)(main_input)\n",
    "#model.add(Flatten())\n",
    "\n",
    "# lstm features\n",
    "lstm_encoding = LSTM(100)(x)\n",
    "\n",
    "# cnn features \n",
    "cnn_mod = Sequential()\n",
    "cnn_mod.add(Conv1D(filters=32, kernel_size=5, padding='same', activation='relu', input_shape=(300,300)))\n",
    "cnn_mod.add(Flatten())\n",
    "cnn_encoding = cnn_mod(x)\n",
    "\n",
    "# combined features\n",
    "merged = concatenate([lstm_encoding, cnn_encoding])\n",
    "\n",
    "#batch_normalized = BatchNormalization()(merged)\n",
    "\n",
    "\n",
    "\n",
    "hidden1 = Dense(300, activation='relu')(merged)\n",
    "\n",
    "#hidden2 = Dense(3)(hidden1)\n",
    "output = Dense(3, activation = 'softmax')(hidden1)\n",
    "\n",
    "\n",
    "model = Model(inputs = main_input, outputs = output)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17621 samples, validate on 1958 samples\n",
      "Epoch 1/100\n",
      "17621/17621 [==============================] - 257s 15ms/step - loss: 0.6874 - val_loss: 0.5048\n",
      "Epoch 2/100\n",
      "17621/17621 [==============================] - 245s 14ms/step - loss: 0.2927 - val_loss: 0.5152\n",
      "Epoch 3/100\n",
      "17621/17621 [==============================] - 245s 14ms/step - loss: 0.1149 - val_loss: 0.6983\n",
      "Epoch 4/100\n",
      "17621/17621 [==============================] - 245s 14ms/step - loss: 0.0328 - val_loss: 0.9857\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb11095ef10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(padded_docs_train, y=ytrain_enc, batch_size=64, epochs=100, \n",
    "          verbose=1, validation_data=(padded_docs_valid, yvalid_enc), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## variation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main_input (InputLayer)         (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_74 (Embedding)        (None, 300, 300)     3000000     main_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_71 (LSTM)                  (None, 100)          160400      embedding_74[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "sequential_134 (Sequential)     (None, 9600)         48032       embedding_74[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "sequential_135 (Sequential)     (None, 30000)        150100      embedding_74[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_29 (Concatenate)    (None, 39700)        0           lstm_71[0][0]                    \n",
      "                                                                 sequential_134[1][0]             \n",
      "                                                                 sequential_135[1][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_105 (Dense)               (None, 400)          15880400    concatenate_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_106 (Dense)               (None, 3)            1203        dense_105[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 19,240,135\n",
      "Trainable params: 19,180,135\n",
      "Non-trainable params: 60,000\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "## next try adding one more feature using dense layers and merge\n",
    "\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "main_input = Input(shape=(300,), dtype='int32', name='main_input')\n",
    "\n",
    "# embedding vectors\n",
    "x = Embedding(vocab_size, 300, input_length=max_length)(main_input)\n",
    "#model.add(Flatten())\n",
    "x1 = flatten(x)\n",
    "\n",
    "# lstm features\n",
    "# lstm_encoding = LSTM(100, return_sequences=False)(x)\n",
    "lstm_encoding = LSTM(100)(x)\n",
    "\n",
    "# cnn features \n",
    "cnn_mod = Sequential()\n",
    "cnn_mod.add(Conv1D(filters=32, kernel_size=5, padding='same', activation='relu', input_shape=(300,300)))\n",
    "cnn_mod.add(Flatten())\n",
    "cnn_encoding = cnn_mod(x)\n",
    "\n",
    "# dense features\n",
    "dense_mod = Sequential()\n",
    "dense_mod.add(TimeDistributed(Dense(100),input_shape=(300,300)))\n",
    "dense_mod.add(Flatten())\n",
    "dense_mod.add(Dropout(0.2))\n",
    "dense_mod.add(BatchNormalization())\n",
    "\n",
    "dense_mod.add(Activation('relu'))\n",
    "#dense_mod.add(Dropout(0.3))\n",
    "# dense_mod.add(BatchNormalization())\n",
    "\n",
    "\n",
    "dense_encoding = dense_mod(x)\n",
    "#dense_encoding1 = flatten(dense_encoding)\n",
    "\n",
    "# combined features\n",
    "merged = concatenate([lstm_encoding, cnn_encoding, dense_encoding])\n",
    "\n",
    "#batch_normalized = BatchNormalization()(merged)\n",
    "\n",
    "\n",
    "\n",
    "hidden1 = Dense(400, activation='relu')(merged)\n",
    "\n",
    "#hidden2 = Dense(3)(hidden1)\n",
    "output = Dense(3, activation = 'softmax')(hidden1)\n",
    "\n",
    "\n",
    "model = Model(inputs = main_input, outputs = output)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17621 samples, validate on 1958 samples\n",
      "Epoch 1/100\n",
      "17621/17621 [==============================] - 294s 17ms/step - loss: 5.3998 - val_loss: 4.1705\n",
      "Epoch 2/100\n",
      "17621/17621 [==============================] - 288s 16ms/step - loss: 5.1103 - val_loss: 5.2190\n",
      "Epoch 3/100\n",
      "17621/17621 [==============================] - 290s 16ms/step - loss: 5.0321 - val_loss: 5.3015\n",
      "Epoch 4/100\n",
      "17621/17621 [==============================] - 295s 17ms/step - loss: 5.0050 - val_loss: 5.4121\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb0ad711e50>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(padded_docs_train, y=ytrain_enc, batch_size=64, epochs=100, \n",
    "          verbose=1, validation_data=(padded_docs_valid, yvalid_enc), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## variation 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### glove vectors creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:27, 14633.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# http://www-nlp.stanford.edu/data/glove.840B.300d.zip\n",
    "# load the GloVe vectors in a dictionary:\n",
    "\n",
    "embeddings_index = {}\n",
    "#f = open('glove.840B.300d.txt')\n",
    "f = open('glove.6B.300d.txt')\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function creates a normalized vector for the whole sentence\n",
    "def sent2vec(s):\n",
    "    words = str(s).lower().decode('utf-8')\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(300)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17621/17621 [00:06<00:00, 2542.54it/s]\n",
      "100%|██████████| 1958/1958 [00:00<00:00, 2563.76it/s]\n"
     ]
    }
   ],
   "source": [
    "# create sentence vectors using the above function for training and validation set\n",
    "xtrain_glove = [sent2vec(x) for x in tqdm(xtrain)]\n",
    "xvalid_glove = [sent2vec(x) for x in tqdm(xvalid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtrain_glove = np.array(xtrain_glove)\n",
    "xvalid_glove = np.array(xvalid_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scale the data before any neural net:\n",
    "scl = preprocessing.StandardScaler()\n",
    "xtrain_glove_scl = scl.fit_transform(xtrain_glove)\n",
    "xvalid_glove_scl = scl.transform(xvalid_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_110 (Dense)            (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dropout_58 (Dropout)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_77 (Batc (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_111 (Dense)            (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dropout_59 (Dropout)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_78 (Batc (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_112 (Dense)            (None, 3)                 903       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 183,903\n",
      "Trainable params: 182,703\n",
      "Non-trainable params: 1,200\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# create a simple 3 layer sequential neural net\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(300, input_dim=300, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(300, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17621 samples, validate on 1958 samples\n",
      "Epoch 1/100\n",
      "17621/17621 [==============================] - 5s 288us/step - loss: 1.2746 - val_loss: 1.1037\n",
      "Epoch 2/100\n",
      "17621/17621 [==============================] - 2s 140us/step - loss: 1.1112 - val_loss: 1.0820\n",
      "Epoch 3/100\n",
      "17621/17621 [==============================] - 2s 140us/step - loss: 1.0765 - val_loss: 1.0898\n",
      "Epoch 4/100\n",
      "17621/17621 [==============================] - 2s 137us/step - loss: 1.0735 - val_loss: 1.0845\n",
      "Epoch 5/100\n",
      "17621/17621 [==============================] - 2s 138us/step - loss: 1.0686 - val_loss: 1.0839\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb09ad27390>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(padded_docs_train, y=ytrain_enc, batch_size=64, epochs=100, \n",
    "          verbose=1, validation_data=(padded_docs_valid, yvalid_enc), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## variation 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main_input (InputLayer)         (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_85 (Embedding)        (None, 300, 300)     3000000     main_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_76 (Flatten)            (None, 90000)        0           embedding_85[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "auxiliary_input (InputLayer)    (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_38 (Concatenate)    (None, 90300)        0           flatten_76[0][0]                 \n",
      "                                                                 auxiliary_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_117 (Dense)               (None, 500)          45150500    concatenate_38[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_118 (Dense)               (None, 3)            1503        dense_117[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 48,152,003\n",
      "Trainable params: 48,152,003\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "## use two kinds of encoding the keras word embedding and glove\n",
    "## use simple architecture for this\n",
    "\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "main_input = Input(shape=(300,), dtype='int32', name='main_input')\n",
    "\n",
    "# embedding vectors\n",
    "x = Embedding(vocab_size, 300, input_length=max_length)(main_input)\n",
    "x1= Flatten()(x)\n",
    "#model.add(Flatten())\n",
    "#x1 = flatten(x)\n",
    "\n",
    "#glove-vectors\n",
    "auxiliary_input = Input(shape=(300,), name='auxiliary_input')\n",
    "\n",
    "#combine the inputs\n",
    "comb_x = concatenate([x1, auxiliary_input])\n",
    "\n",
    "\n",
    "#batch_normalized = BatchNormalization()(comb_x)\n",
    "\n",
    "hidden1 = Dense(500, activation='relu')(comb_x)\n",
    "\n",
    "#hidden2 = Dense(3)(hidden1)\n",
    "output = Dense(3, activation = 'softmax')(hidden1)\n",
    "\n",
    "\n",
    "model = Model(inputs =[main_input,auxiliary_input], outputs = output)\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17621 samples, validate on 1958 samples\n",
      "Epoch 1/100\n",
      "17621/17621 [==============================] - 118s 7ms/step - loss: 1.0915 - val_loss: 0.5236\n",
      "Epoch 2/100\n",
      "17621/17621 [==============================] - 116s 7ms/step - loss: 0.2886 - val_loss: 0.4994\n",
      "Epoch 3/100\n",
      "17621/17621 [==============================] - 115s 7ms/step - loss: 0.0711 - val_loss: 0.6098\n",
      "Epoch 4/100\n",
      "17621/17621 [==============================] - 115s 7ms/step - loss: 0.0160 - val_loss: 0.7167\n",
      "Epoch 5/100\n",
      "17621/17621 [==============================] - 115s 7ms/step - loss: 0.0042 - val_loss: 0.7787\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb099748ed0>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit({'main_input':padded_docs_train, 'auxiliary_input':xtrain_glove_scl}, y=ytrain_enc, batch_size=64, epochs=100, \n",
    "          verbose=1, validation_data=([padded_docs_valid,xvalid_glove_scl], yvalid_enc), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## variation 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## combine the above with cnn , lstm and dense layers\n",
    "\n",
    "## use two kinds of encoding the keras word embedding and glove\n",
    "## use complex architecture for this\n",
    "\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "main_input = Input(shape=(300,), dtype='int32', name='main_input')\n",
    "\n",
    "# embedding vectors\n",
    "x = Embedding(vocab_size, 300, input_length=max_length)(main_input)\n",
    "x1= Flatten()(x)\n",
    "#glove-vectors\n",
    "auxiliary_input = Input(shape=(300,), name='auxiliary_input')\n",
    "\n",
    "#combine the inputs\n",
    "comb_x = concatenate([x1, auxiliary_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 is incompatible with layer lstm_93: expected ndim=3, found ndim=2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-166-82dac3495994>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# lstm features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlstm_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomb_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# cnn features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcnn_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/purr/anaconda2/lib/python2.7/site-packages/keras/layers/recurrent.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/purr/anaconda2/lib/python2.7/site-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m                 \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m                 \u001b[0;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m                 \u001b[0;31m# Collect input shapes to build layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/purr/anaconda2/lib/python2.7/site-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    456\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m                                      str(K.ndim(x)))\n\u001b[0m\u001b[1;32m    459\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m                 \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 is incompatible with layer lstm_93: expected ndim=3, found ndim=2"
     ]
    }
   ],
   "source": [
    "# lstm features\n",
    "lstm_encoding = LSTM(100,return_sequences = True)(comb_x)\n",
    "\n",
    "# cnn features \n",
    "cnn_mod = Sequential()\n",
    "cnn_mod.add(Conv1D(filters=32, kernel_size=5, padding='same', activation='relu', input_shape=(300,300)))\n",
    "cnn_mod.add(Flatten())\n",
    "cnn_encoding = cnn_mod(comb_x)\n",
    "\n",
    "\n",
    "# dense features\n",
    "dense_mod = Sequential()\n",
    "dense_mod.add(TimeDistributed(Dense(100),input_shape=(300,300)))\n",
    "dense_mod.add(Flatten())\n",
    "dense_mod.add(Dropout(0.2))\n",
    "dense_mod.add(BatchNormalization())\n",
    "\n",
    "dense_mod.add(Activation('relu'))\n",
    "#dense_mod.add(Dropout(0.3))\n",
    "# dense_mod.add(BatchNormalization())\n",
    "dense_encoding = dense_mod(comb_x)\n",
    "\n",
    "\n",
    "# combined features\n",
    "merged = concatenate([lstm_encoding, dense_encoding])\n",
    "\n",
    "#batch_normalized = BatchNormalization()(merged)\n",
    "\n",
    "hidden1 = Dense(200, activation='relu')(merged)\n",
    "\n",
    "#hidden2 = Dense(3)(hidden1)\n",
    "output = Dense(3, activation = 'softmax')(hidden1)\n",
    "\n",
    "\n",
    "model = Model(inputs =[main_input,aux_input], outputs = output)\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit({'main_input':padded_docs_train, 'auxiliary_input':xtrain_glove_scl}, y=ytrain_enc, batch_size=64, epochs=100, \n",
    "          verbose=1, validation_data=([padded_docs_valid,xvalid_glove_scl], yvalid_enc), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# variation 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adding dense features as well\n",
    "# we have already seen this is not going to improve our scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
