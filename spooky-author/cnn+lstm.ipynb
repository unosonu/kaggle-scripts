{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spooky author identification challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This challenge invites kagglers to identify the horror story author from the given text snippets. The challenge is little different from other nlp problems because we need to find the signature of the author from his writing style than simply understanding the context vectors. Hence word vectors may be of little help here. I am using keras embeddings instead of word2vec and combine features from LSTM and CNN(to be able to find pattern translation) and then pass the combined features to another hidden layer in the neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "y = lbl_enc.fit_transform(train.author.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtrain, xvalid, ytrain, yvalid = train_test_split(train.text.values, y, \n",
    "                                                  stratify=y, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# binarize the labels for neural net\n",
    "from keras.utils import np_utils\n",
    "ytrain_enc = np_utils.to_categorical(ytrain)\n",
    "yvalid_enc = np_utils.to_categorical(yvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "xtrain_enc = [one_hot(d, vocab_size) for d in xtrain]\n",
    "xvalid_enc = [one_hot(d, vocab_size) for d in xvalid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_length = 300\n",
    "padded_docs_train = pad_sequences(xtrain_enc, maxlen=max_length, padding='post')\n",
    "padded_docs_valid = pad_sequences(xvalid_enc, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Create a functional api for shared features from cnn and lstm\n",
    "\n",
    "#lstm with cnn\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "main_input = Input(shape=(300,), dtype='int32', name='main_input')\n",
    "\n",
    "# embedding vectors\n",
    "x = Embedding(vocab_size, 300, input_length=max_length)(main_input)\n",
    "#model.add(Flatten())\n",
    "\n",
    "# lstm features\n",
    "lstm_encoding = LSTM(100)(x)\n",
    "\n",
    "# cnn features \n",
    "cnn_mod = Sequential()\n",
    "cnn_mod.add(Conv1D(filters=32, kernel_size=5, padding='same', activation='relu', input_shape=(300,300)))\n",
    "cnn_mod.add(Flatten())\n",
    "cnn_encoding = cnn_mod(x)\n",
    "\n",
    "# combined features\n",
    "merged = concatenate([lstm_encoding, cnn_encoding])\n",
    "\n",
    "#batch_normalized = BatchNormalization()(merged)\n",
    "\n",
    "\n",
    "\n",
    "hidden1 = Dense(500, activation='relu')(merged)\n",
    "\n",
    "#hidden2 = Dense(3)(hidden1)\n",
    "output = Dense(3, activation = 'softmax')(hidden1)\n",
    "\n",
    "\n",
    "model = Model(inputs = main_input, outputs = output)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(padded_docs_train, y=ytrain_enc, batch_size=64, epochs=100, \n",
    "          verbose=1, validation_data=(padded_docs_valid, yvalid_enc), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## variation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## next try adding one more feature using dense layers and merge\n",
    "\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "main_input = Input(shape=(300,), dtype='int32', name='main_input')\n",
    "\n",
    "# embedding vectors\n",
    "x = Embedding(vocab_size, 300, input_length=max_length)(main_input)\n",
    "#model.add(Flatten())\n",
    "\n",
    "# lstm features\n",
    "lstm_encoding = LSTM(100)(x)\n",
    "\n",
    "# cnn features \n",
    "cnn_mod = Sequential()\n",
    "cnn_mod.add(Conv1D(filters=32, kernel_size=5, padding='same', activation='relu', input_shape=(300,300)))\n",
    "cnn_mod.add(Flatten())\n",
    "cnn_encoding = cnn_mod(x)\n",
    "\n",
    "# dense features\n",
    "# dense feature\n",
    "dense_mod = Sequential()\n",
    "dense_mod.add(Dense(300, input_dim=300, activation='relu'))\n",
    "dense_mod.add(Dropout(0.2))\n",
    "dense_mod.add(BatchNormalization())\n",
    "\n",
    "dense_mod.add(Dense(300, activation='relu'))\n",
    "dense_mod.add(Dropout(0.3))\n",
    "dense_mod.add(BatchNormalization())\n",
    "dense_encoding = dense_mod(x)\n",
    "\n",
    "\n",
    "# combined features\n",
    "merged = concatenate([lstm_encoding, cnn_encoding, dense_encoding])\n",
    "\n",
    "#batch_normalized = BatchNormalization()(merged)\n",
    "\n",
    "\n",
    "\n",
    "hidden1 = Dense(400, activation='relu')(merged)\n",
    "\n",
    "#hidden2 = Dense(3)(hidden1)\n",
    "output = Dense(3, activation = 'softmax')(hidden1)\n",
    "\n",
    "\n",
    "model = Model(inputs = main_input, outputs = output)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(padded_docs_train, y=ytrain_enc, batch_size=64, epochs=100, \n",
    "          verbose=1, validation_data=(padded_docs_valid, yvalid_enc), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## variation 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### glove vectors creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the GloVe vectors in a dictionary:\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('glove.840B.300d.txt')\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function creates a normalized vector for the whole sentence\n",
    "def sent2vec(s):\n",
    "    words = str(s).lower().decode('utf-8')\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(300)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create sentence vectors using the above function for training and validation set\n",
    "xtrain_glove = [sent2vec(x) for x in tqdm(xtrain)]\n",
    "xvalid_glove = [sent2vec(x) for x in tqdm(xvalid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtrain_glove = np.array(xtrain_glove)\n",
    "xvalid_glove = np.array(xvalid_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scale the data before any neural net:\n",
    "scl = preprocessing.StandardScaler()\n",
    "xtrain_glove_scl = scl.fit_transform(xtrain_glove)\n",
    "xvalid_glove_scl = scl.transform(xvalid_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a simple 3 layer sequential neural net\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(300, input_dim=300, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(300, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(padded_docs_train, y=ytrain_enc, batch_size=64, epochs=100, \n",
    "          verbose=1, validation_data=(padded_docs_valid, yvalid_enc), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## variation 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## use two kinds of encoding the keras word embedding and glove\n",
    "## use simple architecture for this\n",
    "\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "main_input = Input(shape=(300,), dtype='int32', name='main_input')\n",
    "\n",
    "# embedding vectors\n",
    "x = Embedding(vocab_size, 300, input_length=max_length)(main_input)\n",
    "#model.add(Flatten())\n",
    "\n",
    "#glove-vectors\n",
    "auxiliary_input = Input(shape=(300,), name='aux_input')\n",
    "\n",
    "#combine the inputs\n",
    "comb_x = keras.layers.concatenate([x, auxiliary_input])\n",
    "\n",
    "\n",
    "#batch_normalized = BatchNormalization()(comb_x)\n",
    "\n",
    "hidden1 = Dense(500, activation='relu')(comb_x)\n",
    "\n",
    "#hidden2 = Dense(3)(hidden1)\n",
    "output = Dense(3, activation = 'softmax')(hidden1)\n",
    "\n",
    "\n",
    "model = Model(inputs =[main_input,aux_input], outputs = output)\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit({'main_input':padded_docs_train, 'aux_input':xtrain_glove_scl}, y=ytrain_enc, batch_size=64, epochs=100, \n",
    "          verbose=1, validation_data=([padded_docs_valid,yvalid_glove_scl], yvalid_enc), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## variation 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## combine the above with cnn , lstm and dense layers\n",
    "\n",
    "## use two kinds of encoding the keras word embedding and glove\n",
    "## use complex architecture for this\n",
    "\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "main_input = Input(shape=(300,), dtype='int32', name='main_input')\n",
    "\n",
    "# embedding vectors\n",
    "x = Embedding(vocab_size, 300, input_length=max_length)(main_input)\n",
    "#model.add(Flatten())\n",
    "\n",
    "#glove-vectors\n",
    "auxiliary_input = Input(shape=(300,), name='aux_input')\n",
    "\n",
    "#combine the inputs\n",
    "comb_x = keras.layers.concatenate([x, auxiliary_input])\n",
    "\n",
    "# lstm features\n",
    "lstm_encoding = LSTM(100)(comb_x)\n",
    "\n",
    "# cnn features \n",
    "cnn_mod = Sequential()\n",
    "cnn_mod.add(Conv1D(filters=32, kernel_size=5, padding='same', activation='relu', input_shape=(300,300)))\n",
    "cnn_mod.add(Flatten())\n",
    "cnn_encoding = cnn_mod(comb_x)\n",
    "\n",
    "# combined features\n",
    "merged = concatenate([lstm_encoding, cnn_encoding])\n",
    "\n",
    "#batch_normalized = BatchNormalization()(merged)\n",
    "\n",
    "hidden1 = Dense(500, activation='relu')(merged)\n",
    "\n",
    "#hidden2 = Dense(3)(hidden1)\n",
    "output = Dense(3, activation = 'softmax')(hidden1)\n",
    "\n",
    "\n",
    "model = Model(inputs =[main_input,aux_input], outputs = output)\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit({'main_input':padded_docs_train, 'aux_input':xtrain_glove_scl}, y=ytrain_enc, batch_size=64, epochs=100, \n",
    "          verbose=1, validation_data=([padded_docs_valid,yvalid_glove_scl], yvalid_enc), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# variation 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adding dense features as well\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
